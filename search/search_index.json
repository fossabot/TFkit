{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TFKit lets everyone make use of transformer architecture on many tasks and models in small change of config. At the same time, it can do multi-task multi-model learning, and can introduce its own data sets and tasks through simple modifications. Feature \u00b6 One-click replacement of different pre-trained models Support multi-model and multi-task Classifier with multiple labels and multiple classifications Unify input formats for different tasks Separation of data reading and model architecture Support various loss function and indicators Supplement \u00b6 Model list : Support Bert/GPT/GPT2/XLM/XLNet/RoBERTa/CTRL/ALBert/... NLPrep : download and preprocessing data in one line nlp2go : create demo api as quickly as possible. Quick Start \u00b6 Installing via pip \u00b6 pip install tfkit Running TFKit to train a ner model \u00b6 download dataset using nlprep nlprep --dataset tag_clner --outdir ./clner_row --util s2t train model with albert tfkit-train --batch 10 \\ --epoch 3 \\ --lr 5e-6 \\ --train ./clner_row/train \\ --test ./clner_row/test \\ --maxlen 512 \\ --model tagRow \\ --savedir ./albert_ner \\ --config voidful/albert_chinese_small eval model tfkit-eval --model ./albert_ner/3.pt --valid ./clner_row/validation --metric clas host prediction service nlp2go --model ./albert_ner/3.pt --api_path ner Overview \u00b6 Train \u00b6 $ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch BATCH batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch EPOCH epoch, default 10 --maxlen MAXLEN max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --tag TAG [TAG ...] tag to identity task in multi-task --seed SEED random seed, default 609 --worker WORKER number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume RESUME resume training --cache cache training data Eval \u00b6 $ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --print print each pair of evaluate data --enable_arg_panel enable panel to input argument Contributing \u00b6 Thanks for your interest.There are many ways to contribute to this project. Get started here . License \u00b6 License Icons reference \u00b6 Icons modify from Freepik from www.flaticon.com Icons modify from Nikita Golubev from www.flaticon.com","title":"Home"},{"location":"#feature","text":"One-click replacement of different pre-trained models Support multi-model and multi-task Classifier with multiple labels and multiple classifications Unify input formats for different tasks Separation of data reading and model architecture Support various loss function and indicators","title":"Feature"},{"location":"#supplement","text":"Model list : Support Bert/GPT/GPT2/XLM/XLNet/RoBERTa/CTRL/ALBert/... NLPrep : download and preprocessing data in one line nlp2go : create demo api as quickly as possible.","title":"Supplement"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installing-via-pip","text":"pip install tfkit","title":"Installing via pip"},{"location":"#running-tfkit-to-train-a-ner-model","text":"download dataset using nlprep nlprep --dataset tag_clner --outdir ./clner_row --util s2t train model with albert tfkit-train --batch 10 \\ --epoch 3 \\ --lr 5e-6 \\ --train ./clner_row/train \\ --test ./clner_row/test \\ --maxlen 512 \\ --model tagRow \\ --savedir ./albert_ner \\ --config voidful/albert_chinese_small eval model tfkit-eval --model ./albert_ner/3.pt --valid ./clner_row/validation --metric clas host prediction service nlp2go --model ./albert_ner/3.pt --api_path ner","title":"Running TFKit to train a ner model"},{"location":"#overview","text":"","title":"Overview"},{"location":"#train","text":"$ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch BATCH batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch EPOCH epoch, default 10 --maxlen MAXLEN max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --tag TAG [TAG ...] tag to identity task in multi-task --seed SEED random seed, default 609 --worker WORKER number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume RESUME resume training --cache cache training data","title":"Train"},{"location":"#eval","text":"$ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --print print each pair of evaluate data --enable_arg_panel enable panel to input argument","title":"Eval"},{"location":"#contributing","text":"Thanks for your interest.There are many ways to contribute to this project. Get started here .","title":"Contributing"},{"location":"#license","text":"License","title":"License"},{"location":"#icons-reference","text":"Icons modify from Freepik from www.flaticon.com Icons modify from Nikita Golubev from www.flaticon.com","title":"Icons reference"},{"location":"benchmark/","text":"DRCD \u00b6 Test \u00b6 model EM F1 albert-small 74.45% 86.08% electra-small 76.64% 87.49% albert-base 80.17% 89.87% Dev \u00b6 model EM F1 albert-small 73.70% 85.33% electra-small 77.61% 87.33% albert-base 80.52% 89.92%","title":"Benchmark"},{"location":"benchmark/#drcd","text":"","title":"DRCD"},{"location":"benchmark/#test","text":"model EM F1 albert-small 74.45% 86.08% electra-small 76.64% 87.49% albert-base 80.17% 89.87%","title":"Test"},{"location":"benchmark/#dev","text":"model EM F1 albert-small 73.70% 85.33% electra-small 77.61% 87.33% albert-base 80.52% 89.92%","title":"Dev"},{"location":"class/","text":"\u00b6 \u00b6 load_model ( model_path , model_type = None , model_dataset = None ) \u00b6 load model from dumped file Source code in tfkit/eval.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def load_model ( model_path , model_type = None , model_dataset = None ): \"\"\"load model from dumped file\"\"\" device = 'cuda' if torch . cuda . is_available () else 'cpu' torchpack = torch . load ( model_path , map_location = device ) print ( \"===model info===\" ) [ print ( key , ':' , torchpack [ key ]) for key in torchpack . keys () if 'state_dict' not in key and 'models' not in key ] print ( '==========' ) if 'tags' in torchpack and len ( torchpack [ 'tags' ]) > 1 : if model_type is None : print ( \"Pick which models to use in multi-task models\" ) inquirer_res = inquirer . prompt ( [ inquirer . List ( 'model_type' , message = \"Select model\" , choices = torchpack [ 'tags' ])]) model_type = inquirer_res [ 'model_type' ] type_ind = torchpack [ 'tags' ] . index ( model_type ) else : type_ind = 0 print ( \"loading model from dumped file\" ) # get all loading parameter maxlen = torchpack [ 'maxlen' ] config = torchpack [ 'model_config' ] if 'model_config' in torchpack else torchpack [ 'bert' ] model_types = [ torchpack [ 'type' ]] if not isinstance ( torchpack [ 'type' ], list ) else torchpack [ 'type' ] models_state = torchpack [ 'models' ] if 'models' in torchpack else [ torchpack [ 'model_state_dict' ]] type = model_types [ type_ind ] if model_type is None else model_type # load model if 'albert_chinese' in config : tokenizer = BertTokenizer . from_pretrained ( config ) else : tokenizer = AutoTokenizer . from_pretrained ( config ) pretrained = AutoModel . from_pretrained ( config ) type = type . lower () if \"once\" in type : eval_dataset = gen_once . get_data_from_file ( model_dataset ) if model_dataset else None model = gen_once . Once ( tokenizer , pretrained , maxlen = maxlen ) elif \"twice\" in type : eval_dataset = gen_once . get_data_from_file ( model_dataset ) if model_dataset else None model = gen_twice . Twice ( tokenizer , pretrained , maxlen = maxlen ) elif \"onebyone\" in type : eval_dataset = gen_once . get_data_from_file ( model_dataset ) if model_dataset else None model = gen_onebyone . OneByOne ( tokenizer , pretrained , maxlen = maxlen ) elif 'classify' in type or 'clas' in type : eval_dataset = classifier . get_data_from_file ( model_dataset ) if model_dataset else None model = classifier . MtClassifier ( torchpack [ 'task' ], tokenizer , pretrained ) elif 'tag' in type : if model_dataset and \"row\" in type : eval_dataset = tag . get_data_from_file_row ( model_dataset ) elif model_dataset and \"col\" in type : eval_dataset = tag . get_data_from_file_col ( model_dataset ) else : eval_dataset = None model = tag . Tagger ( torchpack [ 'label' ], tokenizer , pretrained , maxlen = maxlen ) elif 'qa' in type : eval_dataset = qa . get_data_from_file ( model_dataset ) if model_dataset else None model = qa . QA ( tokenizer , pretrained , maxlen = maxlen ) model = model . to ( device ) model . load_state_dict ( models_state [ type_ind ], strict = False ) print ( \"finish loading\" ) if model_dataset : return model , eval_dataset else : return model load_predict_parameter ( model , enable_arg_panel = False ) \u00b6 use inquirer panel to let user input model parameter or just use default value Source code in tfkit/eval.py 117 118 119 def load_predict_parameter ( model , enable_arg_panel = False ): \"\"\"use inquirer panel to let user input model parameter or just use default value\"\"\" return nlp2 . function_argument_panel ( model . predict , disable_input_panel = ( not enable_arg_panel ), func_parent = model ) \u00b6","title":"Class"},{"location":"class/#tfkit.train","text":"","title":"tfkit.train"},{"location":"class/#tfkit.eval","text":"","title":"tfkit.eval"},{"location":"class/#tfkit.eval.load_model","text":"load model from dumped file Source code in tfkit/eval.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 def load_model ( model_path , model_type = None , model_dataset = None ): \"\"\"load model from dumped file\"\"\" device = 'cuda' if torch . cuda . is_available () else 'cpu' torchpack = torch . load ( model_path , map_location = device ) print ( \"===model info===\" ) [ print ( key , ':' , torchpack [ key ]) for key in torchpack . keys () if 'state_dict' not in key and 'models' not in key ] print ( '==========' ) if 'tags' in torchpack and len ( torchpack [ 'tags' ]) > 1 : if model_type is None : print ( \"Pick which models to use in multi-task models\" ) inquirer_res = inquirer . prompt ( [ inquirer . List ( 'model_type' , message = \"Select model\" , choices = torchpack [ 'tags' ])]) model_type = inquirer_res [ 'model_type' ] type_ind = torchpack [ 'tags' ] . index ( model_type ) else : type_ind = 0 print ( \"loading model from dumped file\" ) # get all loading parameter maxlen = torchpack [ 'maxlen' ] config = torchpack [ 'model_config' ] if 'model_config' in torchpack else torchpack [ 'bert' ] model_types = [ torchpack [ 'type' ]] if not isinstance ( torchpack [ 'type' ], list ) else torchpack [ 'type' ] models_state = torchpack [ 'models' ] if 'models' in torchpack else [ torchpack [ 'model_state_dict' ]] type = model_types [ type_ind ] if model_type is None else model_type # load model if 'albert_chinese' in config : tokenizer = BertTokenizer . from_pretrained ( config ) else : tokenizer = AutoTokenizer . from_pretrained ( config ) pretrained = AutoModel . from_pretrained ( config ) type = type . lower () if \"once\" in type : eval_dataset = gen_once . get_data_from_file ( model_dataset ) if model_dataset else None model = gen_once . Once ( tokenizer , pretrained , maxlen = maxlen ) elif \"twice\" in type : eval_dataset = gen_once . get_data_from_file ( model_dataset ) if model_dataset else None model = gen_twice . Twice ( tokenizer , pretrained , maxlen = maxlen ) elif \"onebyone\" in type : eval_dataset = gen_once . get_data_from_file ( model_dataset ) if model_dataset else None model = gen_onebyone . OneByOne ( tokenizer , pretrained , maxlen = maxlen ) elif 'classify' in type or 'clas' in type : eval_dataset = classifier . get_data_from_file ( model_dataset ) if model_dataset else None model = classifier . MtClassifier ( torchpack [ 'task' ], tokenizer , pretrained ) elif 'tag' in type : if model_dataset and \"row\" in type : eval_dataset = tag . get_data_from_file_row ( model_dataset ) elif model_dataset and \"col\" in type : eval_dataset = tag . get_data_from_file_col ( model_dataset ) else : eval_dataset = None model = tag . Tagger ( torchpack [ 'label' ], tokenizer , pretrained , maxlen = maxlen ) elif 'qa' in type : eval_dataset = qa . get_data_from_file ( model_dataset ) if model_dataset else None model = qa . QA ( tokenizer , pretrained , maxlen = maxlen ) model = model . to ( device ) model . load_state_dict ( models_state [ type_ind ], strict = False ) print ( \"finish loading\" ) if model_dataset : return model , eval_dataset else : return model","title":"load_model()"},{"location":"class/#tfkit.eval.load_predict_parameter","text":"use inquirer panel to let user input model parameter or just use default value Source code in tfkit/eval.py 117 118 119 def load_predict_parameter ( model , enable_arg_panel = False ): \"\"\"use inquirer panel to let user input model parameter or just use default value\"\"\" return nlp2 . function_argument_panel ( model . predict , disable_input_panel = ( not enable_arg_panel ), func_parent = model )","title":"load_predict_parameter()"},{"location":"class/#tfkit.dump","text":"","title":"tfkit.dump"},{"location":"installation/","text":"Installation \u00b6 tfkit is tested on Python 3.6+, and PyTorch 1.1.0+. Installing via pip \u00b6 pip install tfkit Installing via source \u00b6 git clone https://github.com/voidful/tfkit.git python setup.py install Running tfkit \u00b6 Once you've installed tfkit, you can run with pip installed version: \u00b6 tfkit-train tfkit-eval tfkit-dump local version: \u00b6 python -m tfkit.train python -m tfkit.eval python -m tfkit.dump","title":"Installation"},{"location":"installation/#installation","text":"tfkit is tested on Python 3.6+, and PyTorch 1.1.0+.","title":"Installation"},{"location":"installation/#installing-via-pip","text":"pip install tfkit","title":"Installing via pip"},{"location":"installation/#installing-via-source","text":"git clone https://github.com/voidful/tfkit.git python setup.py install","title":"Installing via source"},{"location":"installation/#running-tfkit","text":"Once you've installed tfkit, you can run with","title":"Running tfkit"},{"location":"installation/#pip-installed-version","text":"tfkit-train tfkit-eval tfkit-dump","title":"pip installed version:"},{"location":"installation/#local-version","text":"python -m tfkit.train python -m tfkit.eval python -m tfkit.dump","title":"local version:"},{"location":"task/","text":"Dataset format \u00b6 once \u00b6 example file csv file with 2 row - input, target each token separate by space no header needed Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\" onebyone \u00b6 example file csv file with 2 row - input, target each token separate by space no header needed Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\" qa \u00b6 example file csv file with 3 row - input, start_pos, end_pos each token separate by space no header needed Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 \u6703 \u4e2d , \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u8868 \u4e86 \u4e0b \u9762 \u9019 \u6bb5 \u8457 \u540d \u7684 \u8a00 \u8ad6 : \u300c \u68b5 \u8a9e \u5118 \u7ba1 \u975e \u5e38 \u53e4 \u8001 , \u69cb \u9020 \u537b \u7cbe \u5999 \u7d55 \u502b : \u6bd4 \u5e0c \u81d8 \u8a9e \u9084 \u5b8c \u7f8e , \u6bd4 \u62c9 \u4e01 \u8a9e \u9084 \u8c50 \u5bcc , \u7cbe \u7dfb \u4e4b \u8655 \u540c \u6642 \u52dd \u904e \u6b64 \u5169 \u8005 , \u4f46 \u5728 \u52d5 \u8a5e \u8a5e \u6839 \u548c \u8a9e \u6cd5 \u5f62 \u5f0f \u4e0a , \u53c8 \u8ddf \u6b64 \u5169 \u8005 \u7121 \u6bd4 \u76f8 \u4f3c , \u4e0d \u53ef \u80fd \u662f \u5de7 \u5408 \u7684 \u7d50 \u679c \u3002 \u9019 \u4e09 \u7a2e \u8a9e \u8a00 \u592a \u76f8 \u4f3c \u4e86 , \u4f7f \u4efb \u4f55 \u540c \u6642 \u7a3d \u8003 \u4e09 \u8005 \u7684 \u8a9e \u6587 \u5b78 \u5bb6 \u90fd \u4e0d \u5f97 \u4e0d \u76f8 \u4fe1 \u4e09 \u8005 \u540c \u51fa \u4e00 \u6e90 , \u51fa \u81ea \u4e00 \u7a2e \u53ef \u80fd \u5df2 \u7d93 \u6d88 \u901d \u7684 \u8a9e \u8a00 \u3002 \u57fa \u65bc \u76f8 \u4f3c \u7684 \u539f \u56e0 , \u5118 \u7ba1 \u7f3a \u5c11 \u540c \u6a23 \u6709 \u529b \u7684 \u8b49 \u64da , \u6211 \u5011 \u53ef \u4ee5 \u63a8 \u60f3 \u54e5 \u5fb7 \u8a9e \u548c \u51f1 \u723e \u7279 \u8a9e , \u96d6 \u7136 \u6df7 \u5165 \u4e86 \u8fe5 \u7136 \u4e0d \u540c \u7684 \u8a9e \u5f59 , \u4e5f \u8207 \u68b5 \u8a9e \u6709 \u8457 \u76f8 \u540c \u7684 \u8d77 \u6e90 ; \u800c \u53e4 \u6ce2 \u65af \u8a9e \u53ef \u80fd \u4e5f \u662f \u9019 \u4e00 \u8a9e \u7cfb \u7684 \u5b50 \u88d4 \u3002 \u300d [Question] \u5370 \u6b50 \u8a9e \u7cfb \u56e0 \u70ba \u54ea \u4e00 \u9580 \u8a9e \u8a00 \u800c \u88ab \u767c \u73fe ?\",47,49 classify \u00b6 example file csv file with header header - input,task1,task2...taskN if some task have multiple label, use / to separate each label - label1/label2/label3 Example: SENTENCE,LABEL,Task2 \"The prospective ultrasound findings were correlated with the final diagnoses , laparotomy findings , and pathology findings .\",outcome/other,1 tagRow \u00b6 example file csv file with 2 row - input, target each token separate by space no header needed Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 [SEP] \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u4e86 \u54ea \u4e00 \u5730 \u5340 \u5c0d \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 ?\",O A A O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O tagCol \u00b6 example file csv file with 2 row - input, target each token separate by space no header needed Example: \u5225 O \u53ea O \u80fd R \u60f3 O \u81ea O \u5df1 O \uff0c O \u60f3 M \u4f60 M \u5468 O \u570d O \u7684 O \u4eba O \u3002 O","title":"Task"},{"location":"task/#dataset-format","text":"","title":"Dataset format"},{"location":"task/#once","text":"example file csv file with 2 row - input, target each token separate by space no header needed Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\"","title":"once"},{"location":"task/#onebyone","text":"example file csv file with 2 row - input, target each token separate by space no header needed Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\"","title":"onebyone"},{"location":"task/#qa","text":"example file csv file with 3 row - input, start_pos, end_pos each token separate by space no header needed Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 \u6703 \u4e2d , \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u8868 \u4e86 \u4e0b \u9762 \u9019 \u6bb5 \u8457 \u540d \u7684 \u8a00 \u8ad6 : \u300c \u68b5 \u8a9e \u5118 \u7ba1 \u975e \u5e38 \u53e4 \u8001 , \u69cb \u9020 \u537b \u7cbe \u5999 \u7d55 \u502b : \u6bd4 \u5e0c \u81d8 \u8a9e \u9084 \u5b8c \u7f8e , \u6bd4 \u62c9 \u4e01 \u8a9e \u9084 \u8c50 \u5bcc , \u7cbe \u7dfb \u4e4b \u8655 \u540c \u6642 \u52dd \u904e \u6b64 \u5169 \u8005 , \u4f46 \u5728 \u52d5 \u8a5e \u8a5e \u6839 \u548c \u8a9e \u6cd5 \u5f62 \u5f0f \u4e0a , \u53c8 \u8ddf \u6b64 \u5169 \u8005 \u7121 \u6bd4 \u76f8 \u4f3c , \u4e0d \u53ef \u80fd \u662f \u5de7 \u5408 \u7684 \u7d50 \u679c \u3002 \u9019 \u4e09 \u7a2e \u8a9e \u8a00 \u592a \u76f8 \u4f3c \u4e86 , \u4f7f \u4efb \u4f55 \u540c \u6642 \u7a3d \u8003 \u4e09 \u8005 \u7684 \u8a9e \u6587 \u5b78 \u5bb6 \u90fd \u4e0d \u5f97 \u4e0d \u76f8 \u4fe1 \u4e09 \u8005 \u540c \u51fa \u4e00 \u6e90 , \u51fa \u81ea \u4e00 \u7a2e \u53ef \u80fd \u5df2 \u7d93 \u6d88 \u901d \u7684 \u8a9e \u8a00 \u3002 \u57fa \u65bc \u76f8 \u4f3c \u7684 \u539f \u56e0 , \u5118 \u7ba1 \u7f3a \u5c11 \u540c \u6a23 \u6709 \u529b \u7684 \u8b49 \u64da , \u6211 \u5011 \u53ef \u4ee5 \u63a8 \u60f3 \u54e5 \u5fb7 \u8a9e \u548c \u51f1 \u723e \u7279 \u8a9e , \u96d6 \u7136 \u6df7 \u5165 \u4e86 \u8fe5 \u7136 \u4e0d \u540c \u7684 \u8a9e \u5f59 , \u4e5f \u8207 \u68b5 \u8a9e \u6709 \u8457 \u76f8 \u540c \u7684 \u8d77 \u6e90 ; \u800c \u53e4 \u6ce2 \u65af \u8a9e \u53ef \u80fd \u4e5f \u662f \u9019 \u4e00 \u8a9e \u7cfb \u7684 \u5b50 \u88d4 \u3002 \u300d [Question] \u5370 \u6b50 \u8a9e \u7cfb \u56e0 \u70ba \u54ea \u4e00 \u9580 \u8a9e \u8a00 \u800c \u88ab \u767c \u73fe ?\",47,49","title":"qa"},{"location":"task/#classify","text":"example file csv file with header header - input,task1,task2...taskN if some task have multiple label, use / to separate each label - label1/label2/label3 Example: SENTENCE,LABEL,Task2 \"The prospective ultrasound findings were correlated with the final diagnoses , laparotomy findings , and pathology findings .\",outcome/other,1","title":"classify"},{"location":"task/#tagrow","text":"example file csv file with 2 row - input, target each token separate by space no header needed Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 [SEP] \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u4e86 \u54ea \u4e00 \u5730 \u5340 \u5c0d \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 ?\",O A A O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O","title":"tagRow"},{"location":"task/#tagcol","text":"example file csv file with 2 row - input, target each token separate by space no header needed Example: \u5225 O \u53ea O \u80fd R \u60f3 O \u81ea O \u5df1 O \uff0c O \u60f3 M \u4f60 M \u5468 O \u570d O \u7684 O \u4eba O \u3002 O","title":"tagCol"},{"location":"usage/","text":"Usage \u00b6 Overview \u00b6 Flow Train \u00b6 $ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch BATCH batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch EPOCH epoch, default 10 --maxlen MAXLEN max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --tag TAG [TAG ...] tag to identity task in multi-task --seed SEED random seed, default 609 --worker WORKER number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume RESUME resume training --cache cache training data Eval \u00b6 $ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --print print each pair of evaluate data --enable_arg_panel enable panel to input argument Example \u00b6 Use distilbert to train NER Model \u00b6 nlprep --dataset tag_clner --outdir ./clner_row --util s2t tfkit-train --batch 10 --epoch 3 --lr 5e-6 --train ./clner_row/train --test ./clner_row/test --maxlen 512 --model tagRow --config distilbert-base-multilingual-cased nlp2go --model ./checkpoints/3.pt --cli Use Albert to train DRCD QA Model \u00b6 nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 512 --savedir ./drcd_qa_model/ --train ./zhqa/drcd-train --test ./zhqa/drcd-test --model qa --config voidful/albert_chinese_small --cache nlp2go --model ./drcd_qa_model/3.pt --cli Use Albert to train both DRCD QA and NER Model \u00b6 nlprep --dataset tag_clner --outdir ./clner_row --util s2t nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 300 --savedir ./mt-qaner --train ./clner_row/train ./zhqa/drcd-train --test ./clner_row/test ./zhqa/drcd-test --model tagRow qa --config voidful/albert_chinese_small nlp2go --model ./mt-qaner/3.pt --cli","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#overview","text":"Flow","title":"Overview"},{"location":"usage/#train","text":"$ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch BATCH batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch EPOCH epoch, default 10 --maxlen MAXLEN max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --tag TAG [TAG ...] tag to identity task in multi-task --seed SEED random seed, default 609 --worker WORKER number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume RESUME resume training --cache cache training data","title":"Train"},{"location":"usage/#eval","text":"$ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --print print each pair of evaluate data --enable_arg_panel enable panel to input argument","title":"Eval"},{"location":"usage/#example","text":"","title":"Example"},{"location":"usage/#use-distilbert-to-train-ner-model","text":"nlprep --dataset tag_clner --outdir ./clner_row --util s2t tfkit-train --batch 10 --epoch 3 --lr 5e-6 --train ./clner_row/train --test ./clner_row/test --maxlen 512 --model tagRow --config distilbert-base-multilingual-cased nlp2go --model ./checkpoints/3.pt --cli","title":"Use distilbert to train NER Model"},{"location":"usage/#use-albert-to-train-drcd-qa-model","text":"nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 512 --savedir ./drcd_qa_model/ --train ./zhqa/drcd-train --test ./zhqa/drcd-test --model qa --config voidful/albert_chinese_small --cache nlp2go --model ./drcd_qa_model/3.pt --cli","title":"Use Albert to train DRCD QA Model"},{"location":"usage/#use-albert-to-train-both-drcd-qa-and-ner-model","text":"nlprep --dataset tag_clner --outdir ./clner_row --util s2t nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 300 --savedir ./mt-qaner --train ./clner_row/train ./zhqa/drcd-train --test ./clner_row/test ./zhqa/drcd-test --model tagRow qa --config voidful/albert_chinese_small nlp2go --model ./mt-qaner/3.pt --cli","title":"Use Albert to train both DRCD QA and NER Model"}]}