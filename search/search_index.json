{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"TFKit lets everyone make use of transformer architecture on many tasks and models in small change of config. At the same time, it can do multi-task multi-model learning, and can introduce its own data sets and tasks through simple modifications. Feature \u00b6 One-click replacement of different pre-trained models Support multi-model and multi-task Classifier with multiple labels and multiple classifications Unify input formats for different tasks Separation of data reading and model architecture Support various loss function and indicators Supplement \u00b6 Model list : Support Bert/GPT/GPT2/XLM/XLNet/RoBERTa/CTRL/ALBert/... NLPrep : download and preprocessing data in one line nlp2go : create demo api as quickly as possible. Quick Start \u00b6 Installing via pip \u00b6 pip install tfkit Running TFKit to train a ner model \u00b6 install nlprep and nlp2go pip install nlprep nlp2go -U download dataset using nlprep nlprep --dataset tag_clner --outdir ./clner_row --util s2t train model with albert tfkit-train --batch 20 \\ --epoch 5 \\ --lr 5e-5 \\ --train ./clner_row/train.csv \\ --test ./clner_row/test.csv \\ --maxlen 512 \\ --model tagRow \\ --savedir ./albert_ner \\ --config voidful/albert_chinese_small eval model tfkit-eval --model ./albert_ner/3.pt --valid ./clner_row/validation.csv --metric clas result Task : default report TASK: default 0 precision recall f1-score support B_Abstract 0.00 0.00 0.00 1 B_Location 1.00 1.00 1.00 1 B_Metric 1.00 1.00 1.00 1 B_Organization 0.00 0.00 0.00 1 B_Person 1.00 1.00 1.00 1 B_Physical 0.00 0.00 0.00 1 B_Thing 1.00 1.00 1.00 1 B_Time 1.00 1.00 1.00 1 I_Abstract 1.00 1.00 1.00 1 I_Location 1.00 1.00 1.00 1 I_Metric 1.00 1.00 1.00 1 I_Organization 0.00 0.00 0.00 1 I_Person 1.00 1.00 1.00 1 I_Physical 0.00 0.00 0.00 1 I_Thing 1.00 1.00 1.00 1 I_Time 1.00 1.00 1.00 1 O 1.00 1.00 1.00 1 micro avg 1.00 0.71 0.83 17 macro avg 0.71 0.71 0.71 17 weighted avg 0.71 0.71 0.71 17 samples avg 1.00 0.71 0.83 17 host prediction service nlp2go --model ./albert_ner/3.pt --api_path ner You can also try tfkit in Google Colab: Overview \u00b6 Train \u00b6 $ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch BATCH batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch EPOCH epoch, default 10 --maxlen MAXLEN max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --tag TAG [TAG ...] tag to identity task in multi-task --seed SEED random seed, default 609 --worker WORKER number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume RESUME resume training --cache cache training data Eval \u00b6 $ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --print print each pair of evaluate data --enable_arg_panel enable panel to input argument Contributing \u00b6 Thanks for your interest.There are many ways to contribute to this project. Get started here . License \u00b6 License Icons reference \u00b6 Icons modify from Freepik from www.flaticon.com Icons modify from Nikita Golubev from www.flaticon.com","title":"Home"},{"location":"#feature","text":"One-click replacement of different pre-trained models Support multi-model and multi-task Classifier with multiple labels and multiple classifications Unify input formats for different tasks Separation of data reading and model architecture Support various loss function and indicators","title":"Feature"},{"location":"#supplement","text":"Model list : Support Bert/GPT/GPT2/XLM/XLNet/RoBERTa/CTRL/ALBert/... NLPrep : download and preprocessing data in one line nlp2go : create demo api as quickly as possible.","title":"Supplement"},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installing-via-pip","text":"pip install tfkit","title":"Installing via pip"},{"location":"#running-tfkit-to-train-a-ner-model","text":"install nlprep and nlp2go pip install nlprep nlp2go -U download dataset using nlprep nlprep --dataset tag_clner --outdir ./clner_row --util s2t train model with albert tfkit-train --batch 20 \\ --epoch 5 \\ --lr 5e-5 \\ --train ./clner_row/train.csv \\ --test ./clner_row/test.csv \\ --maxlen 512 \\ --model tagRow \\ --savedir ./albert_ner \\ --config voidful/albert_chinese_small eval model tfkit-eval --model ./albert_ner/3.pt --valid ./clner_row/validation.csv --metric clas result Task : default report TASK: default 0 precision recall f1-score support B_Abstract 0.00 0.00 0.00 1 B_Location 1.00 1.00 1.00 1 B_Metric 1.00 1.00 1.00 1 B_Organization 0.00 0.00 0.00 1 B_Person 1.00 1.00 1.00 1 B_Physical 0.00 0.00 0.00 1 B_Thing 1.00 1.00 1.00 1 B_Time 1.00 1.00 1.00 1 I_Abstract 1.00 1.00 1.00 1 I_Location 1.00 1.00 1.00 1 I_Metric 1.00 1.00 1.00 1 I_Organization 0.00 0.00 0.00 1 I_Person 1.00 1.00 1.00 1 I_Physical 0.00 0.00 0.00 1 I_Thing 1.00 1.00 1.00 1 I_Time 1.00 1.00 1.00 1 O 1.00 1.00 1.00 1 micro avg 1.00 0.71 0.83 17 macro avg 0.71 0.71 0.71 17 weighted avg 0.71 0.71 0.71 17 samples avg 1.00 0.71 0.83 17 host prediction service nlp2go --model ./albert_ner/3.pt --api_path ner You can also try tfkit in Google Colab:","title":"Running TFKit to train a ner model"},{"location":"#overview","text":"","title":"Overview"},{"location":"#train","text":"$ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch BATCH batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch EPOCH epoch, default 10 --maxlen MAXLEN max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --tag TAG [TAG ...] tag to identity task in multi-task --seed SEED random seed, default 609 --worker WORKER number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume RESUME resume training --cache cache training data","title":"Train"},{"location":"#eval","text":"$ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --print print each pair of evaluate data --enable_arg_panel enable panel to input argument","title":"Eval"},{"location":"#contributing","text":"Thanks for your interest.There are many ways to contribute to this project. Get started here .","title":"Contributing"},{"location":"#license","text":"License","title":"License"},{"location":"#icons-reference","text":"Icons modify from Freepik from www.flaticon.com Icons modify from Nikita Golubev from www.flaticon.com","title":"Icons reference"},{"location":"benchmark/","text":"DRCD \u00b6 Test \u00b6 model EM F1 albert-small 74.45% 86.08% electra-small 76.64% 87.49% albert-base 80.17% 89.87% Dev \u00b6 model EM F1 albert-small 73.70% 85.33% electra-small 77.61% 87.33% albert-base 80.52% 89.92%","title":"Benchmark"},{"location":"benchmark/#drcd","text":"","title":"DRCD"},{"location":"benchmark/#test","text":"model EM F1 albert-small 74.45% 86.08% electra-small 76.64% 87.49% albert-base 80.17% 89.87%","title":"Test"},{"location":"benchmark/#dev","text":"model EM F1 albert-small 73.70% 85.33% electra-small 77.61% 87.33% albert-base 80.52% 89.92%","title":"Dev"},{"location":"class/","text":"\u00b6 \u00b6","title":"Class"},{"location":"class/#tfkit.train","text":"","title":"tfkit.train"},{"location":"class/#tfkit.eval","text":"","title":"tfkit.eval"},{"location":"installation/","text":"Installation \u00b6 tfkit is tested on Python 3.6+, and PyTorch 1.1.0+. Installing via pip \u00b6 pip install tfkit Installing via source \u00b6 git clone https://github.com/voidful/tfkit.git python setup.py install Running tfkit \u00b6 Model you've installed tfkit, you can run with pip installed version: \u00b6 tfkit-train tfkit-eval tfkit-dump local version: \u00b6 python -m tfkit.train python -m tfkit.eval python -m tfkit.dump","title":"Installation"},{"location":"installation/#installation","text":"tfkit is tested on Python 3.6+, and PyTorch 1.1.0+.","title":"Installation"},{"location":"installation/#installing-via-pip","text":"pip install tfkit","title":"Installing via pip"},{"location":"installation/#installing-via-source","text":"git clone https://github.com/voidful/tfkit.git python setup.py install","title":"Installing via source"},{"location":"installation/#running-tfkit","text":"Model you've installed tfkit, you can run with","title":"Running tfkit"},{"location":"installation/#pip-installed-version","text":"tfkit-train tfkit-eval tfkit-dump","title":"pip installed version:"},{"location":"installation/#local-version","text":"python -m tfkit.train python -m tfkit.eval python -m tfkit.dump","title":"local version:"},{"location":"task/","text":"Task format \u00b6 All data will be in csv format No header needed Each token have to separate by space once \u00b6 example file Format: input, target Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\" onebyone \u00b6 example file Format: input, target Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\" qa \u00b6 example file Format: input, start_pos, end_pos Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 \u6703 \u4e2d , \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u8868 \u4e86 \u4e0b \u9762 \u9019 \u6bb5 \u8457 \u540d \u7684 \u8a00 \u8ad6 : \u300c \u68b5 \u8a9e \u5118 \u7ba1 \u975e \u5e38 \u53e4 \u8001 , \u69cb \u9020 \u537b \u7cbe \u5999 \u7d55 \u502b : \u6bd4 \u5e0c \u81d8 \u8a9e \u9084 \u5b8c \u7f8e , \u6bd4 \u62c9 \u4e01 \u8a9e \u9084 \u8c50 \u5bcc , \u7cbe \u7dfb \u4e4b \u8655 \u540c \u6642 \u52dd \u904e \u6b64 \u5169 \u8005 , \u4f46 \u5728 \u52d5 \u8a5e \u8a5e \u6839 \u548c \u8a9e \u6cd5 \u5f62 \u5f0f \u4e0a , \u53c8 \u8ddf \u6b64 \u5169 \u8005 \u7121 \u6bd4 \u76f8 \u4f3c , \u4e0d \u53ef \u80fd \u662f \u5de7 \u5408 \u7684 \u7d50 \u679c \u3002 \u9019 \u4e09 \u7a2e \u8a9e \u8a00 \u592a \u76f8 \u4f3c \u4e86 , \u4f7f \u4efb \u4f55 \u540c \u6642 \u7a3d \u8003 \u4e09 \u8005 \u7684 \u8a9e \u6587 \u5b78 \u5bb6 \u90fd \u4e0d \u5f97 \u4e0d \u76f8 \u4fe1 \u4e09 \u8005 \u540c \u51fa \u4e00 \u6e90 , \u51fa \u81ea \u4e00 \u7a2e \u53ef \u80fd \u5df2 \u7d93 \u6d88 \u901d \u7684 \u8a9e \u8a00 \u3002 \u57fa \u65bc \u76f8 \u4f3c \u7684 \u539f \u56e0 , \u5118 \u7ba1 \u7f3a \u5c11 \u540c \u6a23 \u6709 \u529b \u7684 \u8b49 \u64da , \u6211 \u5011 \u53ef \u4ee5 \u63a8 \u60f3 \u54e5 \u5fb7 \u8a9e \u548c \u51f1 \u723e \u7279 \u8a9e , \u96d6 \u7136 \u6df7 \u5165 \u4e86 \u8fe5 \u7136 \u4e0d \u540c \u7684 \u8a9e \u5f59 , \u4e5f \u8207 \u68b5 \u8a9e \u6709 \u8457 \u76f8 \u540c \u7684 \u8d77 \u6e90 ; \u800c \u53e4 \u6ce2 \u65af \u8a9e \u53ef \u80fd \u4e5f \u662f \u9019 \u4e00 \u8a9e \u7cfb \u7684 \u5b50 \u88d4 \u3002 \u300d [Question] \u5370 \u6b50 \u8a9e \u7cfb \u56e0 \u70ba \u54ea \u4e00 \u9580 \u8a9e \u8a00 \u800c \u88ab \u767c \u73fe ?\",47,49 classify \u00b6 example file Format: input,target_0,target_1..... If some task have multiple label, use / to separate each label - label1/label2/label3 Example: \"The prospective ultrasound findings were correlated with the final diagnoses , laparotomy findings , and pathology findings .\",outcome/other,1 tagRow \u00b6 example file Format: input, target Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 [SEP] \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u4e86 \u54ea \u4e00 \u5730 \u5340 \u5c0d \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 ?\",O A A O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O tagCol \u00b6 example file Format: input, target Example: \u5225 O \u53ea O \u80fd R \u60f3 O \u81ea O \u5df1 O \uff0c O \u60f3 M \u4f60 M \u5468 O \u570d O \u7684 O \u4eba O \u3002 O","title":"Task"},{"location":"task/#task-format","text":"All data will be in csv format No header needed Each token have to separate by space","title":"Task format"},{"location":"task/#once","text":"example file Format: input, target Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\"","title":"once"},{"location":"task/#onebyone","text":"example file Format: input, target Example: \"i go to school by bus\",\"\u6211 \u5750 \u5df4 \u58eb \u4e0a \u5b78\"","title":"onebyone"},{"location":"task/#qa","text":"example file Format: input, start_pos, end_pos Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 \u6703 \u4e2d , \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u8868 \u4e86 \u4e0b \u9762 \u9019 \u6bb5 \u8457 \u540d \u7684 \u8a00 \u8ad6 : \u300c \u68b5 \u8a9e \u5118 \u7ba1 \u975e \u5e38 \u53e4 \u8001 , \u69cb \u9020 \u537b \u7cbe \u5999 \u7d55 \u502b : \u6bd4 \u5e0c \u81d8 \u8a9e \u9084 \u5b8c \u7f8e , \u6bd4 \u62c9 \u4e01 \u8a9e \u9084 \u8c50 \u5bcc , \u7cbe \u7dfb \u4e4b \u8655 \u540c \u6642 \u52dd \u904e \u6b64 \u5169 \u8005 , \u4f46 \u5728 \u52d5 \u8a5e \u8a5e \u6839 \u548c \u8a9e \u6cd5 \u5f62 \u5f0f \u4e0a , \u53c8 \u8ddf \u6b64 \u5169 \u8005 \u7121 \u6bd4 \u76f8 \u4f3c , \u4e0d \u53ef \u80fd \u662f \u5de7 \u5408 \u7684 \u7d50 \u679c \u3002 \u9019 \u4e09 \u7a2e \u8a9e \u8a00 \u592a \u76f8 \u4f3c \u4e86 , \u4f7f \u4efb \u4f55 \u540c \u6642 \u7a3d \u8003 \u4e09 \u8005 \u7684 \u8a9e \u6587 \u5b78 \u5bb6 \u90fd \u4e0d \u5f97 \u4e0d \u76f8 \u4fe1 \u4e09 \u8005 \u540c \u51fa \u4e00 \u6e90 , \u51fa \u81ea \u4e00 \u7a2e \u53ef \u80fd \u5df2 \u7d93 \u6d88 \u901d \u7684 \u8a9e \u8a00 \u3002 \u57fa \u65bc \u76f8 \u4f3c \u7684 \u539f \u56e0 , \u5118 \u7ba1 \u7f3a \u5c11 \u540c \u6a23 \u6709 \u529b \u7684 \u8b49 \u64da , \u6211 \u5011 \u53ef \u4ee5 \u63a8 \u60f3 \u54e5 \u5fb7 \u8a9e \u548c \u51f1 \u723e \u7279 \u8a9e , \u96d6 \u7136 \u6df7 \u5165 \u4e86 \u8fe5 \u7136 \u4e0d \u540c \u7684 \u8a9e \u5f59 , \u4e5f \u8207 \u68b5 \u8a9e \u6709 \u8457 \u76f8 \u540c \u7684 \u8d77 \u6e90 ; \u800c \u53e4 \u6ce2 \u65af \u8a9e \u53ef \u80fd \u4e5f \u662f \u9019 \u4e00 \u8a9e \u7cfb \u7684 \u5b50 \u88d4 \u3002 \u300d [Question] \u5370 \u6b50 \u8a9e \u7cfb \u56e0 \u70ba \u54ea \u4e00 \u9580 \u8a9e \u8a00 \u800c \u88ab \u767c \u73fe ?\",47,49","title":"qa"},{"location":"task/#classify","text":"example file Format: input,target_0,target_1..... If some task have multiple label, use / to separate each label - label1/label2/label3 Example: \"The prospective ultrasound findings were correlated with the final diagnoses , laparotomy findings , and pathology findings .\",outcome/other,1","title":"classify"},{"location":"task/#tagrow","text":"example file Format: input, target Example: \"\u5728 \u6b50 \u6d32 , \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 , \u7531 \u5fb7 \u570b \u5b78 \u8005 \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u3002 \u5f8c \u4f86 \u5a01 \u5ec9 \u00b7 \u74ca \u65af \u767c \u73fe \u5370 \u6b50 \u8a9e \u7cfb , \u4e5f \u8981 \u6b78 \u529f \u65bc \u5c0d \u68b5 \u8a9e \u7684 \u7814 \u7a76 \u3002 \u6b64 \u5916 , \u68b5 \u8a9e \u7814 \u7a76 , \u4e5f \u5c0d \u897f \u65b9 \u6587 \u5b57 \u5b78 \u53ca \u6b77 \u53f2 \u8a9e \u8a00 \u5b78 \u7684 \u767c \u5c55 , \u8ca2 \u737b \u4e0d \u5c11 \u3002 1 7 8 6 \u5e74 2 \u6708 2 \u65e5 , \u4e9e \u6d32 \u5354 \u6703 \u5728 \u52a0 \u723e \u5404 \u7b54 \u8209 \u884c \u3002 [SEP] \u9678 \u7279 \u548c \u6f22 \u65af \u96f7 \u9813 \u958b \u5275 \u4e86 \u54ea \u4e00 \u5730 \u5340 \u5c0d \u68b5 \u8a9e \u7684 \u5b78 \u8853 \u7814 \u7a76 ?\",O A A O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O O","title":"tagRow"},{"location":"task/#tagcol","text":"example file Format: input, target Example: \u5225 O \u53ea O \u80fd R \u60f3 O \u81ea O \u5df1 O \uff0c O \u60f3 M \u4f60 M \u5468 O \u570d O \u7684 O \u4eba O \u3002 O","title":"tagCol"},{"location":"usage/","text":"Usage \u00b6 Overview \u00b6 Flow Train \u00b6 $ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch epoch, default 10 --maxlen max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --add_tokens auto add top x percent UNK token to word table, default 0, range 0-100 --tag [TAG ...] tag to identity task in multi-task --seed random seed, default 609 --worker number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume resume training --cache cache training data Eval \u00b6 $ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --config pre-trained model path after add token --print print each pair of evaluate data --enable_arg_panel enable panel to input argument Example \u00b6 Use distilbert to train NER Model \u00b6 nlprep --dataset tag_clner --outdir ./clner_row --util s2t tfkit-train --batch 10 --epoch 3 --lr 5e-6 --train ./clner_row/train --test ./clner_row/test --maxlen 512 --model tagRow --config distilbert-base-multilingual-cased nlp2go --model ./checkpoints/3.pt --cli Use Albert to train DRCD Model Model \u00b6 nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 512 --savedir ./drcd_qa_model/ --train ./zhqa/drcd-train --test ./zhqa/drcd-test --model qa --config voidful/albert_chinese_small --cache nlp2go --model ./drcd_qa_model/3.pt --cli Use Albert to train both DRCD Model and NER Model \u00b6 nlprep --dataset tag_clner --outdir ./clner_row --util s2t nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 300 --savedir ./mt-qaner --train ./clner_row/train ./zhqa/drcd-train --test ./clner_row/test ./zhqa/drcd-test --model tagRow qa --config voidful/albert_chinese_small nlp2go --model ./mt-qaner/3.pt --cli","title":"Usage"},{"location":"usage/#usage","text":"","title":"Usage"},{"location":"usage/#overview","text":"Flow","title":"Overview"},{"location":"usage/#train","text":"$ tfkit-train Run training arguments: --train TRAIN [TRAIN ...] train dataset path --test TEST [TEST ...] test dataset path --config CONFIG distilbert-base-multilingual-cased/bert-base-multilingual-cased/voidful/albert_chinese_small --model {once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} [{once,twice,onebyone,clas,tagRow,tagCol,qa,onebyone-neg,onebyone-pos,onebyone-both} ...] model task --savedir SAVEDIR model saving dir, default /checkpoints optional arguments: -h, --help show this help message and exit --batch batch size, default 20 --lr LR [LR ...] learning rate, default 5e-5 --epoch epoch, default 10 --maxlen max tokenized sequence length, default 368 --lossdrop loss dropping for text generation --add_tokens auto add top x percent UNK token to word table, default 0, range 0-100 --tag [TAG ...] tag to identity task in multi-task --seed random seed, default 609 --worker number of worker on pre-processing, default 8 --grad_accum gradient accumulation, default 1 --tensorboard Turn on tensorboard graphing --resume resume training --cache cache training data","title":"Train"},{"location":"usage/#eval","text":"$ tfkit-eval Run evaluation on different benchmark arguments: --model MODEL model path --metric {emf1,nlg,clas} evaluate metric --valid VALID evaluate data path optional arguments: -h, --help show this help message and exit --config pre-trained model path after add token --print print each pair of evaluate data --enable_arg_panel enable panel to input argument","title":"Eval"},{"location":"usage/#example","text":"","title":"Example"},{"location":"usage/#use-distilbert-to-train-ner-model","text":"nlprep --dataset tag_clner --outdir ./clner_row --util s2t tfkit-train --batch 10 --epoch 3 --lr 5e-6 --train ./clner_row/train --test ./clner_row/test --maxlen 512 --model tagRow --config distilbert-base-multilingual-cased nlp2go --model ./checkpoints/3.pt --cli","title":"Use distilbert to train NER Model"},{"location":"usage/#use-albert-to-train-drcd-model-model","text":"nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 512 --savedir ./drcd_qa_model/ --train ./zhqa/drcd-train --test ./zhqa/drcd-test --model qa --config voidful/albert_chinese_small --cache nlp2go --model ./drcd_qa_model/3.pt --cli","title":"Use Albert to train DRCD Model Model"},{"location":"usage/#use-albert-to-train-both-drcd-model-and-ner-model","text":"nlprep --dataset tag_clner --outdir ./clner_row --util s2t nlprep --dataset qa_zh --outdir ./zhqa/ tfkit-train --maxlen 300 --savedir ./mt-qaner --train ./clner_row/train ./zhqa/drcd-train --test ./clner_row/test ./zhqa/drcd-test --model tagRow qa --config voidful/albert_chinese_small nlp2go --model ./mt-qaner/3.pt --cli","title":"Use Albert to train both DRCD Model and NER Model"}]}